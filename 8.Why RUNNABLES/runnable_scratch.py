# -*- coding: utf-8 -*-
"""LLM_Runnable.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JMyeucF7Wnbbr45YH5fJsdTzF32vzI7q
"""

from abc import ABC, abstractmethod

class Runnable(ABC):
  def invoke(input_data):
    pass

import random

class FakeLLM(Runnable):
  def __init__(self):
    print('LLM created')
  def invoke(self, prompt):
    response_list = [
        'response 1',
        'response 2',
        'response 3'
    ]
    return {'joke': random.choice(response_list)}

class FakePromptTemplate(Runnable):
  def __init__(self, template, input_variables):
    self.template = template
    self.input_variables = input_variables

  def invoke(self, input):
    return self.template.format(**input)

template = FakePromptTemplate(
    template='{name} is {adjective}',
    input_variables=['name', 'adjective']
)

prompt = template.invoke({'name': 'Bob', 'adjective': 'tall'})
print(prompt)

class RunnableConnector(Runnable):
  def __init__(self, runnable_list):
    self.runnable_list = runnable_list

  def invoke(self, input):
    output = input
    for runnable in self.runnable_list:
      output = runnable.invoke(output)
    return output

class FakeOutputParser(Runnable):
  def __init__(self):
    pass

  def invoke(self, input):
    return input['joke']

llm = FakeLLM()

parser = FakeOutputParser()

chain = RunnableConnector([
    template,
    llm,
    parser
])

# chain.invoke({'name':'sudip', 'adjective'})

template1 = FakePromptTemplate(
    template='write a joke about {name}',
    input_variables=['name']
)
template2 = FakePromptTemplate(
    template='Explain the following joke to me: {joke}',
    input_variables=['joke']
)

chain1 = RunnableConnector([
    template1,
    llm
])
chain2 = RunnableConnector([
    template2,
    llm,
    parser
])
final_chain = RunnableConnector([chain1, chain2])
final_chain.invoke({'name': 'space'})

